# -*- coding: utf-8 -*-
"""final_project_last.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PKW_zGzCMmllHz6rWU0hg1yKC_tr9EkD

<h4> รายชื่อสมาชิก <br>
นาย ศุภกร ทวีชัยนฤมิตร     <b>61102010164</b> <br>
นาย ศุภวิชญ์ จรัสพรศรีวงศ์ <b>61102010165</b> <br>
นาย กันติวิทย์ สุวัฒนมาลา  <b>61102010418</b> <br>
นาย ณัฐิวุฒิ เอกธาราวงศ์    <b>61102010423</b> <br>
</h4>

# Data Explore
"""

# Commented out IPython magic to ensure Python compatibility.
#import library

# %matplotlib inline
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

#ทำการอ่านไฟล์
trained_data = pd.read_csv("/content/train_ctrUa4K.csv")
tested_data = pd.read_csv("/content/test_lAUu6dG.csv")

#copy test file ไว้ทำการใส่ค่าจากการทำนายเพื่อสร้างไฟล์ในภายหลัง
tested_data2=tested_data.copy()

#view first 10 rows of training dataset *จะพบว่ามีค่าNaN อยู่ ต้องแก้ไขในภายหลัง*
trained_data.head(10)

#view first 10 rows of tested dataset *จะพบว่ามีค่าNaN อยู่ ต้องแก้ไขในภายหลัง*
tested_data.head(10)

#show shape for each datasets
trained_data.shape , tested_data.shape

#ดูจำนวนข้อมูลและตัวที่มีค่า NaN ใน train data
trained_data.info()

"""# Exploratory Data Analysis (EDA)





"""

# นับจำนวน Loan_Status column 

trained_data['Loan_Status'].value_counts()

#Count AND Plot Loan_Status's column value
#จะพบว่า422คนจะได้รับการอนุมัติ
trained_data['Loan_Status'].value_counts().plot.bar()

"""## พิจารณาตัวแปรอิสระ ประเภทCategorical"""

trained_data["Gender"].value_counts().plot.bar()
plt.show()
#จะพบว่าส่วนใหญ่ที่สมัครจะเป็นผู้ชาย

trained_data["Married"].value_counts().plot.bar()
plt.show()
#จะพบว่าส่วนใหญ่ที่สมัครจะเป็นผู้ที่แต่งงานแล้ว

trained_data["Self_Employed"].value_counts().plot.bar()
plt.show()
#จะพบว่าส่วนน้อยจะเป็นพวก self_employed

trained_data['Credit_History'].value_counts().plot.bar()
plt.show()
#จะพบว่าส่วนใหญ่ที่สมัครจะเป็นผู้ที่มีเครดิต

"""## พิจารณาตัวแปรอิสระ ประเภทOrdinal"""

trained_data['Dependents'].value_counts().plot.bar()
plt.show()
#ผู้สมัคร ไม่มีdependents มากกว่าครึ่ง

trained_data['Education'].value_counts().plot.bar()
plt.show()
#ผู้สมัครส่วนใหญ่จบการศึกษา

trained_data['Property_Area'].value_counts().plot.bar()
plt.show()
#ผู้สมัครส่วนใหญ่มาจาก Semiurban

"""## พิจารณาตัวแปรอิสระ ประเภทNumeric"""

sns.distplot(trained_data['ApplicantIncome'])
plt.show()
#ApplicantIncome is not normally distributed

sns.distplot(trained_data['CoapplicantIncome'])
plt.show()
#CoapplicantIncome is not normally distributed

trained_data['Loan_Amount_Term'].value_counts()

sns.distplot(trained_data['LoanAmount'])
plt.show()
#LoanAmount  มีค่า outliers ค่อนข้างมาก

"""##Bivariate (Relation of Independent Variable vs Target Variable)


"""

Gen=pd.crosstab(trained_data["Gender"],trained_data["Loan_Status"])
Gen.div(Gen.sum(1).astype(float), axis=0).plot(kind="barh")
plt.show()
#สัดส่วนการอนุมัติและการไม่อนุมัติมีค่าใกล้เคียงกันมาก เมื่อเทียบกับเพศ

Married=pd.crosstab(trained_data["Married"],trained_data['Loan_Status'])
Married.div(Married.sum(1).astype(float), axis=0).plot(kind='barh')
plt.show()
#สัดส่วนการอนุมัติและการไม่อนุมัติมีค่าต่างกันเล็กน้อย เมื่อเทียบกับผู้ที่แต่งงานแล้วกับยังไม่แต่งงาน

Dependents=pd.crosstab(trained_data['Dependents'],trained_data['Loan_Status'])
Dependents.div(Dependents.sum(1).astype(float), axis=0).plot(kind='barh')
plt.show()
#สัดส่วนการอนุมัติและการไม่อนุมัติมีค่าต่างกันแบบหลากลาย เมื่อเทียบกับเงินได้ในส่วนของผู้อยู่ในอุปการะ

Self_Employ=pd.crosstab(trained_data['Self_Employed'],trained_data['Loan_Status'])
Self_Employ.div(Self_Employ.sum(1).astype(float), axis=0).plot(kind='barh')
plt.show()
#สัดส่วนการอนุมัติและการไม่อนุมัติมีค่าใกล้เคียงกันมาก เมื่อเทียบกับรายได้แบบ Self Employed

Edu=pd.crosstab(trained_data['Education'],trained_data['Loan_Status'])
Edu.div(Edu.sum(1).astype(float), axis=0).plot(kind='barh')
plt.show()
#สัดส่วนการอนุมัติและการไม่อนุมัติมีค่าต่างกันเล็กน้อย เมื่อเทียบกับการสำเร็จการศึกษา

CreditHis=pd.crosstab(trained_data["Credit_History"],trained_data['Loan_Status'])
CreditHis.div(CreditHis.sum(1).astype(float), axis=0).plot(kind='barh')
plt.show()
#สัดส่วนการอนุมัติและการไม่อนุมัติมีค่าต่างกันอย่างมาก เมื่อเทียบกับประวัติการใช้งาน

PropertyArea=pd.crosstab(trained_data["Property_Area"],trained_data['Loan_Status'])
PropertyArea.div(PropertyArea.sum(1).astype(float), axis=0).plot(kind='barh')
plt.show()
#สัดส่วนการอนุมัติและการไม่อนุมัติมีค่าต่างกันอย่างมาก เมื่อเทียบกับพื้นที่ที่อาศัย

"""## Numerical (Independent Variable vs Target Variable)"""

#grouped data by Loan_Status and ApplicantIncome
#เพื่อเปรียบเทียบ พบว่ารายได้ของผู้ที่ได้การอนุมัติกับรายได้ของผู้ที่ไม่ได้รับการอนุมัติ มีค่าไม่ต่างกัน

trained_data.groupby("Loan_Status")["ApplicantIncome"].mean().plot(kind="barh")

"""# Impute Missing Value

## Impute Missing Value for train data
"""

#เช็คค่าที่ Null
trained_data.isnull().head(5)

#นับค่าที่ Null ทั้งหมดในแต่ละcolumn ของตัว trained_data
trained_data.isnull().sum()

#ทำการแก้ missing value ของตัวแปรประเภท category ด้วย mode

trained_data['Gender'].fillna(trained_data['Gender'].mode()[0], inplace=True)
trained_data['Married'].fillna(trained_data['Married'].mode()[0], inplace=True)
trained_data['Dependents'].fillna(trained_data['Dependents'].mode()[0], inplace=True)
trained_data['Self_Employed'].fillna(trained_data['Self_Employed'].mode()[0], inplace=True)
trained_data['Credit_History'].fillna(trained_data['Credit_History'].mode()[0], inplace=True)
trained_data['Loan_Amount_Term'].fillna(trained_data['Loan_Amount_Term'].mode()[0], inplace=True)

#เช็คค่า Null อีกรอบพบว่า LoanAmount ยังNullอยู่ ซึ่งเราจะแทนด้วยค่า Median
trained_data.isnull().sum()

#หาค่า median จาก LoanAmount
trained_data['LoanAmount'].median()

#ทำการแก้ missing value ของตัวแปรประเภท numeric ด้วย median
trained_data['LoanAmount'].fillna(trained_data['LoanAmount'].median(), inplace=True)

#check Null value  จะพบว่าไม่มีแล้วพร้อมต่อการสร้างmodel
trained_data.isnull().sum()

"""## Impute Missing Value for test data

"""

#เช็คค่าที่ Null
tested_data.isnull().head(5)

#นับค่าที่ Null ทั้งหมดในแต่ละcolumn ของตัว tested_data
tested_data.isnull().sum()

#ทำการแก้ missing value ของตัวแปรประเภท category ด้วย mode

tested_data['Gender'].fillna(tested_data['Gender'].mode()[0], inplace=True)
tested_data['Married'].fillna(tested_data['Married'].mode()[0], inplace=True)
tested_data['Dependents'].fillna(tested_data['Dependents'].mode()[0], inplace=True)
tested_data['Self_Employed'].fillna(tested_data['Self_Employed'].mode()[0], inplace=True)
tested_data['Credit_History'].fillna(tested_data['Credit_History'].mode()[0], inplace=True)
tested_data['Loan_Amount_Term'].fillna(tested_data['Loan_Amount_Term'].mode()[0], inplace=True)

#เช็คค่า Null อีกรอบพบว่า LoanAmount ยังNullอยู่ ซึ่งเราจะแทนด้วยค่า Median
tested_data.isnull().sum()

#หาค่า median จาก LoanAmount
tested_data['LoanAmount'].median()

#ทำการแก้ missing value ของตัวแปรประเภท numeric ด้วย median
tested_data['LoanAmount'].fillna(trained_data['LoanAmount'].median(), inplace=True)

#Check Null value จะพบว่าไม่มีแล้วพร้อมต่อการสร้างmodel
tested_data.isnull().sum()

"""# Model Building

"""

trained_data=trained_data.drop('Loan_ID',axis=1)
tested_data=tested_data.drop('Loan_ID',axis=1)
#ทำการdrop column Loan_ID ทิ้งเพราะเราไม่ได้ใช้ในการพิจารณา

X = trained_data.drop('Loan_Status',1)
y = trained_data.Loan_Status

X = pd.get_dummies(X)
trained_data=pd.get_dummies(trained_data)
tested_data=pd.get_dummies(tested_data)
#ทำการสร้างตัวแปร dummy

X.shape, trained_data.shape, tested_data.shape

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(X,y, test_size=0.35)
#ทำการแบ่ง train dataset , test dataset โดยแบ่งสัดส่วนเป็น test 35 % , train 65%

#import library ต่างๆที่ต้องใช้
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import StratifiedKFold, cross_val_score,GridSearchCV
from sklearn.metrics import accuracy_score

"""## Tunning Model"""

# เหตุผลที่ใช้ model XGBoost เพราะสามารถใช้ hardwareได้อย่างมีประสิทธิภาพ ,สามารถลดการเกิดoverfitได้ และ มีประสิทธิภาพในการ perdictได้สูง

# ใช้ n_splits=15 ทำการแบ่งข้อมูลเป็น 14-16 ส่วน ค่านี้จะเป็นค่าที่ดีที่สุดในการปรับ เพราะทำให้ได้ค่าคะแนนจากการทำนายที่สูง ถ้าเพิ่มค่าจะได้คะแนนตํ่าลงและถ้าลดค่าก็ทำให้คะแนนตํ่าลง

# random_state=5 ให้ทำการสุ่มทีละ 5 ตัวจะได้คะแนนที่สูงที่สุด เมื่อปรับเป็นเลขอื่นจะทำให้คะแนนจากการทำนายน้อยลงไปด้วย

# shuffle=True ทำการสุ่มเลขโดยใช้การเรียงลำดับ

# n_estimators=70 ได้ปรับจำนวนที่สามารถทำให้scoreสูงที่สุด จะอยู่ในช่วง 70-85 เป็นช่วงที่ทำให้perfomanceดีขึ้น และช่วงนั้นทำให้performanceเริ่มนิ่งแล้ว

# max_depth=4 ความลึกของtree เมื่อปรับที่ 4 จะทำให้ได้ประสิทธิภาพสูงที่สุด

from xgboost import XGBClassifier
j=1 
mean = 0
kf = StratifiedKFold(n_splits=15,random_state=5,shuffle=True) 
for train_index,test_index in kf.split(X,y): 
 xtr,xts = X.loc[train_index],X.loc[test_index] 
 ytr,yts = y[train_index],y[test_index] 
 model = XGBClassifier(n_estimators=70, max_depth=3) 
 model.fit(xtr, ytr) 
 pred_test = model.predict(xts) 
 score = accuracy_score(yts,pred_test) 
 mean += score
 j+=1
 pred_test = model.predict(tested_data)
 pred = model.predict_proba(xts)[:,1]
print ('\n Mean Validation Accuracy',mean/(j-1))
print(pred_test)

#นำค่าที่ได้มา predictกับตัวtest data เพื่อหา accuracy score
pred_cv = model.predict(x_test)
accuracy_score(y_test,pred_cv)

#ทำการสร้างไฟล์ โดยใช้columnของ'Loan_ID','Loan_Status เพื่อทำการไปcheck score ในเว็ป
submissionfile = pd.read_csv('sample_submission.csv')

submissionfile['Loan_Status']=pred_test
submissionfile['Loan_ID']=tested_data2['Loan_ID']
submissionfile['Loan_Status'].replace(0, 'N', inplace=True)
submissionfile['Loan_Status'].replace(1, 'Y', inplace=True)

pd.DataFrame(submissionfile, columns=['Loan_ID','Loan_Status']).to_csv('/content/test_score_last.csv', index=False)